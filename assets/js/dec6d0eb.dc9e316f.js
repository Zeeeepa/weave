"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5582],{44098:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>u,toc:()=>c});var n=r(85893),o=r(11151),a=r(65488),s=r(85162);const i={},l="Models",u={id:"guides/core-types/models",title:"Models",description:"A Model is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates. By structuring your code to be compatible with this API, you benefit from a structured way to version your application so you can more systematically keep track of your experiments.",source:"@site/docs/guides/core-types/models.md",sourceDirName:"guides/core-types",slug:"/guides/core-types/models",permalink:"/guides/core-types/models",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/core-types/models.md",tags:[],version:"current",lastUpdatedAt:174018181e4,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Prompts",permalink:"/guides/core-types/prompts"},next:{title:"Datasets",permalink:"/guides/core-types/datasets"}},d={},c=[{value:"Automatic versioning of models",id:"automatic-versioning-of-models",level:2},{value:"Serve models",id:"serve-models",level:2},{value:"Track production calls",id:"track-production-calls",level:2},{value:"Pairwise evaluation of models",id:"pairwise-evaluation-of-models",level:2}];function p(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"models",children:"Models"}),"\n",(0,n.jsxs)(t.p,{children:["A ",(0,n.jsx)(t.code,{children:"Model"})," is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates. By structuring your code to be compatible with this API, you benefit from a structured way to version your application so you can more systematically keep track of your experiments."]}),"\n",(0,n.jsxs)(a.Z,{groupId:"programming-language",queryString:!0,children:[(0,n.jsxs)(s.default,{value:"python",label:"Python",default:!0,children:[(0,n.jsx)(t.p,{children:"To create a model in Weave, you need the following:"}),(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["a class that inherits from ",(0,n.jsx)(t.code,{children:"weave.Model"})]}),"\n",(0,n.jsx)(t.li,{children:"type definitions on all attributes"}),"\n",(0,n.jsxs)(t.li,{children:["a typed ",(0,n.jsx)(t.code,{children:"predict"})," function with ",(0,n.jsx)(t.code,{children:"@weave.op()"})," decorator"]}),"\n"]}),(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"from weave import Model\nimport weave\n\nclass YourModel(Model):\n    attribute1: str\n    attribute2: int\n\n    @weave.op()\n    def predict(self, input_data: str) -> dict:\n        # Model logic goes here\n        prediction = self.attribute1 + ' ' + input_data\n        return {'pred': prediction}\n"})}),(0,n.jsx)(t.p,{children:"You can call the model as usual with:"}),(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"import weave\nweave.init('intro-example')\n\nmodel = YourModel(attribute1='hello', attribute2=5)\nmodel.predict('world')\n"})}),(0,n.jsxs)(t.p,{children:["This will track the model settings along with the inputs and outputs anytime you call ",(0,n.jsx)(t.code,{children:"predict"}),"."]}),(0,n.jsx)(t.h2,{id:"automatic-versioning-of-models",children:"Automatic versioning of models"}),(0,n.jsx)(t.p,{children:"When you change the attributes or the code that defines your model, these changes will be logged and the version will be updated.\nThis ensures that you can compare the predictions across different versions of your model. Use this to iterate on prompts or to try the latest LLM and compare predictions across different settings."}),(0,n.jsx)(t.p,{children:"For example, here we create a new model:"}),(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"import weave\nweave.init('intro-example')\n\nmodel = YourModel(attribute1='howdy', attribute2=10)\nmodel.predict('world')\n"})}),(0,n.jsx)(t.p,{children:"After calling this, you will see that you now have two versions of this Model in the UI, each with different tracked calls."}),(0,n.jsx)(t.h2,{id:"serve-models",children:"Serve models"}),(0,n.jsx)(t.p,{children:"To serve a model, you can easily spin up a FastAPI server by calling:"}),(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bash",children:"weave serve <your model ref>\n"})}),(0,n.jsxs)(t.p,{children:["For additional instructions, see ",(0,n.jsx)(t.a,{href:"/guides/tools/serve",children:"serve"}),"."]}),(0,n.jsx)(t.h2,{id:"track-production-calls",children:"Track production calls"}),(0,n.jsx)(t.p,{children:"To separate production calls, you can add an additional attribute to the predictions for easy filtering in the UI or API."}),(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"with weave.attributes({'env': 'production'}):\n    model.predict('world')\n"})}),(0,n.jsx)(t.h2,{id:"pairwise-evaluation-of-models",children:"Pairwise evaluation of models"}),(0,n.jsxs)(t.p,{children:["When ",(0,n.jsx)(t.a,{href:"/guides/evaluation/scorers",children:"scoring"})," models in a Weave ",(0,n.jsx)(t.a,{href:"/guides/core-types/evaluations",children:"evaluation"}),", absolute value metrics (e.g. ",(0,n.jsx)(t.code,{children:"9/10"})," for Model A and ",(0,n.jsx)(t.code,{children:"8/10"})," for Model B) are typically harder to assign than than relative ones (e.g. Model A performs better than Model B). ",(0,n.jsx)(t.em,{children:"Pairwise evaluation"})," allows you to compare the outputs of two models by ranking them relative to each other. This approach is particularly useful when you want to determine which model performs better for subjective tasks such as text generation, summarization, or question answering. With pairwise evaluation, you can obtain a relative preference ranking that reveals which model is best for specific inputs."]}),(0,n.jsxs)(t.p,{children:["The following code sample demonstrates how to implement a pairwise evaluation in Weave by creating a ",(0,n.jsx)(t.a,{href:"/guides/evaluation/scorers#class-based-scorers",children:"class-based scorer"})," called ",(0,n.jsx)(t.code,{children:"PreferenceScorer"}),". The ",(0,n.jsx)(t.code,{children:"PreferenceScorer"})," compares two models, ",(0,n.jsx)(t.code,{children:"ModelA"})," and ",(0,n.jsx)(t.code,{children:"ModelB"}),", and returns a relative score of the model outputs based on explicit hints in the input text."]}),(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'from weave import Model, Evaluation, Scorer, Dataset\nfrom weave.flow.model import ApplyModelError, apply_model_async\n\nclass ModelA(Model):\n    @weave.op\n    def predict(self, input_text: str):\n        if "Prefer model A" in input_text:\n            return {"response": "This is a great answer from Model A"}\n        return {"response": "Meh, whatever"}\n\nclass ModelB(Model):\n    @weave.op\n    def predict(self, input_text: str):\n        if "Prefer model B" in input_text:\n            return {"response": "This is a thoughtful answer from Model B"}\n        return {"response": "I don\'t know"}\n\nclass PreferenceScorer(Scorer):\n    @weave.op\n    async def _get_other_model_output(self, example: dict) -> Any:\n        """Get output from the other model for comparison.\n        Args:\n            example: The input example data to run through the other model\n        Returns:\n            The output from the other model\n        """\n\n        other_model_result = await apply_model_async(\n            self.other_model,\n            example,\n            None,\n        )\n\n        if isinstance(other_model_result, ApplyModelError):\n            return None\n\n        return other_model_result.model_output\n\n    @weave.op\n    async def score(self, output: dict, input_text: str) -> dict:\n        """Compare the output of the primary model with the other model.\n        Args:\n            output (dict): The output from the primary model.\n            other_output (dict): The output from the other model being compared.\n            inputs (str): The input text used to generate the outputs.\n        Returns:\n            dict: A flat dictionary containing the comparison result and reason.\n        """\n        other_output = await self._get_other_model_output(\n            {"input_text": inputs}\n        )\n        if other_output is None:\n            return {"primary_is_better": False, "reason": "Other model failed"}\n\n        if "Prefer model A" in input_text:\n            primary_is_better = True\n            reason = "Model A gave a great answer"\n        else:\n            primary_is_better = False\n            reason = "Model B is preferred for this type of question"\n\n        return {"primary_is_better": primary_is_better, "reason": reason}\n\ndataset = Dataset(\n    rows=[\n        {"input_text": "Prefer model A: Question 1"},  # Model A wins\n        {"input_text": "Prefer model A: Question 2"},  # Model A wins\n        {"input_text": "Prefer model B: Question 3"},  # Model B wins\n        {"input_text": "Prefer model B: Question 4"},  # Model B wins\n    ]\n)\n\nmodel_a = ModelA()\nmodel_b = ModelB()\npref_scorer = PreferenceScorer(other_model=model_b)\nevaluation = Evaluation(dataset=dataset, scorers=[pref_scorer])\nevaluation.evaluate(model_a)\n'})})]}),(0,n.jsx)(s.default,{value:"typescript",label:"TypeScript",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-plaintext",children:"This feature is not available in TypeScript yet.  Stay tuned!\n"})})})]})]})}function h(e={}){const{wrapper:t}={...(0,o.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}},85162:(e,t,r)=>{r.r(t),r.d(t,{default:()=>s});r(67294);var n=r(90512);const o={tabItem:"tabItem_Ymn6"};var a=r(85893);function s(e){let{children:t,hidden:r,className:s}=e;return(0,a.jsx)("div",{role:"tabpanel",className:(0,n.Z)(o.tabItem,s),hidden:r,children:t})}},65488:(e,t,r)=>{r.d(t,{Z:()=>h});var n=r(67294),o=r(90512),a=r(12466),s=r(70989),i=r(72389);const l={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var u=r(85893);function d(e){let{className:t,block:r,selectedValue:n,selectValue:s,tabValues:i}=e;const d=[],{blockElementScrollPositionUntilNextRender:c}=(0,a.o5)(),p=e=>{const t=e.currentTarget,r=d.indexOf(t),o=i[r].value;o!==n&&(c(t),s(o))},h=e=>{let t=null;switch(e.key){case"Enter":p(e);break;case"ArrowRight":{const r=d.indexOf(e.currentTarget)+1;t=d[r]??d[0];break}case"ArrowLeft":{const r=d.indexOf(e.currentTarget)-1;t=d[r]??d[d.length-1];break}}t?.focus()};return(0,u.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":r},t),children:i.map((e=>{let{value:t,label:r,attributes:a}=e;return(0,u.jsx)("li",{role:"tab",tabIndex:n===t?0:-1,"aria-selected":n===t,ref:e=>d.push(e),onKeyDown:h,onClick:p,...a,className:(0,o.Z)("tabs__item",l.tabItem,a?.className,{"tabs__item--active":n===t}),children:r??t},t)}))})}function c(e){let{lazy:t,children:r,selectedValue:o}=e;const a=(Array.isArray(r)?r:[r]).filter(Boolean);if(t){const e=a.find((e=>e.props.value===o));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return(0,u.jsx)("div",{className:"margin-top--md",children:a.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==o})))})}function p(e){const t=(0,s.Y)(e);return(0,u.jsxs)("div",{className:(0,o.Z)("tabs-container",l.tabList),children:[(0,u.jsx)(d,{...t,...e}),(0,u.jsx)(c,{...t,...e})]})}function h(e){const t=(0,i.default)();return(0,u.jsx)(p,{...e,children:(0,s.h)(e.children)},String(t))}},70989:(e,t,r)=>{r.d(t,{Y:()=>h,h:()=>u});var n=r(67294),o=r(16550),a=r(20469),s=r(91980),i=r(67392),l=r(20812);function u(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function d(e){const{values:t,children:r}=e;return(0,n.useMemo)((()=>{const e=t??function(e){return u(e).map((e=>{let{props:{value:t,label:r,attributes:n,default:o}}=e;return{value:t,label:r,attributes:n,default:o}}))}(r);return function(e){const t=(0,i.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,r])}function c(e){let{value:t,tabValues:r}=e;return r.some((e=>e.value===t))}function p(e){let{queryString:t=!1,groupId:r}=e;const a=(0,o.k6)(),i=function(e){let{queryString:t=!1,groupId:r}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!r)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return r??null}({queryString:t,groupId:r});return[(0,s._X)(i),(0,n.useCallback)((e=>{if(!i)return;const t=new URLSearchParams(a.location.search);t.set(i,e),a.replace({...a.location,search:t.toString()})}),[i,a])]}function h(e){const{defaultValue:t,queryString:r=!1,groupId:o}=e,s=d(e),[i,u]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:r}=e;if(0===r.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!c({value:t,tabValues:r}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${r.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=r.find((e=>e.default))??r[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:s}))),[h,m]=p({queryString:r,groupId:o}),[f,v]=function(e){let{groupId:t}=e;const r=function(e){return e?`docusaurus.tab.${e}`:null}(t),[o,a]=(0,l.Nk)(r);return[o,(0,n.useCallback)((e=>{r&&a.set(e)}),[r,a])]}({groupId:o}),b=(()=>{const e=h??f;return c({value:e,tabValues:s})?e:null})();(0,a.Z)((()=>{b&&u(b)}),[b]);return{selectedValue:i,selectValue:(0,n.useCallback)((e=>{if(!c({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);u(e),m(e),v(e)}),[m,v,s]),tabValues:s}}},11151:(e,t,r)=>{r.d(t,{Z:()=>i,a:()=>s});var n=r(67294);const o={},a=n.createContext(o);function s(e){const t=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),n.createElement(a.Provider,{value:t},e.children)}}}]);