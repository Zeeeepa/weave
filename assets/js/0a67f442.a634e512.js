"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5409],{73185:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>d});var t=s(85893),o=s(11151);const a={},r="Amazon Bedrock",i={id:"guides/integrations/bedrock",title:"Amazon Bedrock",description:"Weave automatically tracks and logs LLM calls made via Amazon Bedrock, AWS's fully managed service that offers foundation models from leading AI companies through a unified API.",source:"@site/docs/guides/integrations/bedrock.md",sourceDirName:"guides/integrations",slug:"/guides/integrations/bedrock",permalink:"/guides/integrations/bedrock",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/integrations/bedrock.md",tags:[],version:"current",lastUpdatedAt:1737580641e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Microsoft Azure",permalink:"/guides/integrations/azure"},next:{title:"Google Gemini",permalink:"/guides/integrations/google-gemini"}},c={},d=[{value:"Traces",id:"traces",level:2},{value:"Wrapping with your own ops",id:"wrapping-with-your-own-ops",level:2},{value:"Create a <code>Model</code> for easier experimentation",id:"create-a-model-for-easier-experimentation",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",...(0,o.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"amazon-bedrock",children:"Amazon Bedrock"}),"\n",(0,t.jsx)(n.p,{children:"Weave automatically tracks and logs LLM calls made via Amazon Bedrock, AWS's fully managed service that offers foundation models from leading AI companies through a unified API."}),"\n",(0,t.jsx)(n.h2,{id:"traces",children:"Traces"}),"\n",(0,t.jsx)(n.p,{children:"Weave will automatically capture traces for Bedrock API calls. You can use the Bedrock client as usual after initializing Weave and patching the client:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import weave\nimport boto3\nimport json\nfrom weave.integrations.bedrock.bedrock_sdk import patch_client\n\nweave.init("my_bedrock_app")\n\n# Create and patch the Bedrock client\nclient = boto3.client("bedrock-runtime")\npatch_client(client)\n\n# Use the client as usual\nresponse = client.invoke_model(\n    modelId="anthropic.claude-3-5-sonnet-20240620-v1:0",\n    body=json.dumps({\n        "anthropic_version": "bedrock-2023-05-31",\n        "max_tokens": 100,\n        "messages": [\n            {"role": "user", "content": "What is the capital of France?"}\n        ]\n    }),\n    contentType=\'application/json\',\n    accept=\'application/json\'\n)\nresponse_dict = json.loads(response.get(\'body\').read())\nprint(response_dict["content"][0]["text"])\n'})}),"\n",(0,t.jsxs)(n.p,{children:["of using the ",(0,t.jsx)(n.code,{children:"converse"})," API:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'messages = [{"role": "user", "content": [{"text": "What is the capital of France?"}]}]\n\nresponse = client.converse(\n    modelId="anthropic.claude-3-5-sonnet-20240620-v1:0",\n    system=[{"text": "You are a helpful AI assistant."}],\n    messages=messages,\n    inferenceConfig={"maxTokens": 100},\n)\nprint(response["output"]["message"]["content"][0]["text"])\n\n'})}),"\n",(0,t.jsx)(n.h2,{id:"wrapping-with-your-own-ops",children:"Wrapping with your own ops"}),"\n",(0,t.jsxs)(n.p,{children:["You can create reusable operations using the ",(0,t.jsx)(n.code,{children:"@weave.op()"})," decorator. Here's an example showing both the ",(0,t.jsx)(n.code,{children:"invoke_model"})," and ",(0,t.jsx)(n.code,{children:"converse"})," APIs:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'@weave.op\ndef call_model_invoke(\n    model_id: str,\n    prompt: str,\n    max_tokens: int = 100,\n    temperature: float = 0.7\n) -> dict:\n    body = json.dumps({\n        "anthropic_version": "bedrock-2023-05-31",\n        "max_tokens": max_tokens,\n        "temperature": temperature,\n        "messages": [\n            {"role": "user", "content": prompt}\n        ]\n    })\n\n    response = client.invoke_model(\n        modelId=model_id,\n        body=body,\n        contentType=\'application/json\',\n        accept=\'application/json\'\n    )\n    return json.loads(response.get(\'body\').read())\n\n@weave.op\ndef call_model_converse(\n    model_id: str,\n    messages: str,\n    system_message: str,\n    max_tokens: int = 100,\n) -> dict:\n    response = client.converse(\n        modelId=model_id,\n        system=[{"text": system_message}],\n        messages=messages,\n        inferenceConfig={"maxTokens": max_tokens},\n    )\n    return response\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:s(75754).Z+"",width:"3394",height:"1520"})}),"\n",(0,t.jsxs)(n.h2,{id:"create-a-model-for-easier-experimentation",children:["Create a ",(0,t.jsx)(n.code,{children:"Model"})," for easier experimentation"]}),"\n",(0,t.jsxs)(n.p,{children:["You can create a Weave Model to better organize your experiments and capture parameters. Here's an example using the ",(0,t.jsx)(n.code,{children:"converse"})," API:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class BedrockLLM(weave.Model):\n    model_id: str\n    max_tokens: int = 100\n    system_message: str = "You are a helpful AI assistant."\n\n    @weave.op\n    def predict(self, prompt: str) -> str:\n        "Generate a response using Bedrock\'s converse API"\n        \n        messages = [{\n            "role": "user",\n            "content": [{"text": prompt}]\n        }]\n\n        response = client.converse(\n            modelId=self.model_id,\n            system=[{"text": self.system_message}],\n            messages=messages,\n            inferenceConfig={"maxTokens": self.max_tokens},\n        )\n        return response["output"]["message"]["content"][0]["text"]\n\n# Create and use the model\nmodel = BedrockLLM(\n    model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",\n    max_tokens=100,\n    system_message="You are an expert software engineer that knows a lot of programming. You prefer short answers."\n)\nresult = model.predict("What is the best way to handle errors in Python?")\nprint(result)\n'})}),"\n",(0,t.jsx)(n.p,{children:"This approach allows you to version your experiments and easily track different configurations of your Bedrock-based application."})]})}function p(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},75754:(e,n,s)=>{s.d(n,{Z:()=>t});const t=s.p+"assets/images/bedrock_converse-6f9dce63982bc118ac2fab9b4c96f744.png"},11151:(e,n,s)=>{s.d(n,{Z:()=>i,a:()=>r});var t=s(67294);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);