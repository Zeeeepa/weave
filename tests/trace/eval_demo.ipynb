{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "def print_quick_table(rows:list[dict | BaseModel]):\n",
    "    final_rows = []\n",
    "    for row in rows:\n",
    "        if isinstance(row, BaseModel):\n",
    "            final_rows.append(row.model_dump())\n",
    "        else:\n",
    "            final_rows.append(row)\n",
    "    print(tabulate.tabulate(final_rows, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.trace.refs import ObjectRef, TableRef\n",
    "from weave.trace_server_bindings.remote_http_trace_server import RemoteHTTPTraceServer\n",
    "from weave.wandb_interface import wandb_api\n",
    "\n",
    "entity = \"timssweeney\"\n",
    "project = \"on_demand_eval_demo_4\"\n",
    "project_id = f\"{entity}/{project}\"\n",
    "\n",
    "wandb_api.init()\n",
    "wandb_context = wandb_api.get_wandb_api_context()\n",
    "\n",
    "server_client = RemoteHTTPTraceServer(\"http://127.0.01:6345\")\n",
    "server_client.set_auth((\"\", wandb_context.api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.a: Create Dataset\n",
    "#\n",
    "# These low level calls are facilitated by the dataset uploader & edit UIs\n",
    "\n",
    "table = server_client.table_create({\n",
    "    \"table\": {\n",
    "        \"project_id\": project_id,\n",
    "        \"rows\": [\n",
    "            {\"input\": \"United States\"},\n",
    "            {\"input\": \"Canada\"},\n",
    "            {\"input\": \"Mexico\"},\n",
    "        ],\n",
    "    },\n",
    "})\n",
    "table_digest = table.digest\n",
    "table_ref = TableRef(\n",
    "    entity=entity,\n",
    "    project=project,\n",
    "    _digest=table_digest,\n",
    ").uri()\n",
    "\n",
    "dataset = server_client.obj_create({\n",
    "    \"obj\": {\n",
    "        \"project_id\": project_id,\n",
    "        \"object_id\": \"country_abbrev_dataset\",\n",
    "        \"val\": {\"_type\": \"Dataset\", \"_class_name\": \"Dataset\", \"_bases\": [\"Object\", \"BaseModel\"], \"rows\": table_ref},\n",
    "    },\n",
    "})\n",
    "dataset_digest = dataset.digest\n",
    "dataset_ref = ObjectRef(\n",
    "    entity=entity,\n",
    "    project=project,\n",
    "    name=\"country_abbrev_dataset\",\n",
    "    _digest=dataset_digest,\n",
    ").uri()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = server_client.objs_query({\n",
    "    \"project_id\": project_id,\n",
    "    \"filter\": {\n",
    "        \"base_object_classes\": [\"Dataset\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "print_quick_table(datasets.objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.a: Create Scorer\n",
    "#\n",
    "# These low level calls are facilitated by the scorer builder UI\n",
    "\n",
    "scorer = server_client.obj_create({\n",
    "    \"obj\": {\n",
    "        \"project_id\": project_id,\n",
    "        \"object_id\": \"abbreviation_correctness_scorer\",\n",
    "        \"builtin_object_class\": \"LLMJudgeScorer\",\n",
    "        \"val\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"system_prompt\": \"Determine if the output is the correct abbreviation for the given country.\",\n",
    "            \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"response\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"passed\": {\"type\": \"boolean\"},\n",
    "                        \"reason\": {\"type\": \"string\"},\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        },\n",
    "    },\n",
    "})\n",
    "\n",
    "scorer_digest = scorer.digest\n",
    "scorer_ref = ObjectRef(\n",
    "    entity=entity,\n",
    "    project=project,\n",
    "    name=\"correctness_scorer\",\n",
    "    _digest=scorer_digest,\n",
    ").uri()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = server_client.objs_query({\n",
    "    \"project_id\": project_id,\n",
    "    \"filter\": {\n",
    "        \"base_object_classes\": [\"Scorer\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "print_quick_table(scorers.objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.a: Create Evaluation\n",
    "#\n",
    "# These low level calls are facilitated by the evaluation builder UI\n",
    "\n",
    "evaluation = server_client.obj_create({\n",
    "    \"obj\": {\n",
    "        \"project_id\": project_id,\n",
    "        \"object_id\": \"country_abbrev_evaluation\",\n",
    "        \"val\": {\"_type\": \"Evaluation\", \"_class_name\": \"Evaluation\", \"_bases\": [\"Object\", \"BaseModel\"],\n",
    "                \"dataset\": dataset_ref,\n",
    "                \"scorers\": [scorer_ref],\n",
    "                },\n",
    "    },\n",
    "})\n",
    "evaluation_digest = evaluation.digest\n",
    "evaluation_ref = ObjectRef(\n",
    "    entity=entity,\n",
    "    project=project,\n",
    "    name=\"country_abbrev_evaluation\",\n",
    "    _digest=evaluation_digest,\n",
    ").uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = server_client.objs_query({\n",
    "    \"project_id\": project_id,\n",
    "    \"filter\": {\n",
    "        \"base_object_classes\": [\"Evaluation\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "print_quick_table(evaluations.objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.a: Create a model\n",
    "#\n",
    "# These low level calls are facilitated by the model builder UI\n",
    "\n",
    "\n",
    "model = server_client.obj_create({\n",
    "    \"obj\": {\n",
    "        \"project_id\": project_id,\n",
    "        \"object_id\": \"correctness_model\",\n",
    "        \"builtin_object_class\": \"LiteLLMCompletionModel\",\n",
    "        \"val\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"messages_template\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Determine the abbreviation for the given country.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": \"{input}\"},\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "})\n",
    "\n",
    "model_digest = model.digest\n",
    "model_ref = ObjectRef(\n",
    "    entity=entity,\n",
    "    project=project,\n",
    "    name=\"correctness_model\",\n",
    "    _digest=model_digest,\n",
    ").uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.a: Run the evaluation:\n",
    "\n",
    "results = server_client.evaluate_stream({\n",
    "    \"project_id\": project_id,\n",
    "    \"evaluation_ref\": evaluation_ref,\n",
    "    \"model_ref\": model_ref,\n",
    "})\n",
    "\n",
    "async for result in await results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_calls = server_client.calls_query_stream({\n",
    "    \"project_id\": project_id,\n",
    "    \"filter\": {\n",
    "        \"op_names\": [f\"weave:///{project_id}/op/Evaluation.evaluate:*\"],\n",
    "        \"input_refs\": [\n",
    "            evaluation_ref,\n",
    "        ],\n",
    "        \"trace_roots_only\": True,\n",
    "    }\n",
    "})\n",
    "\n",
    "print_quick_table(list(evaluate_calls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import os\n",
    "os.environ[\"WF_TRACE_SERVER_URL\"] = \"http://127.0.01:6345\"\n",
    "correctness_model = weave.ref(\"weave:///timssweeney/on_demand_eval_demo_3/object/correctness_model:3Y1Z6Q6okGNtFunX9OYYqxggIO3MxtFyXkzTPmYFjxA\").get()\n",
    "correctness_model.predict(input=\"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb-weave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
