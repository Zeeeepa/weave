from typing import Any, Optional
from pydantic import BaseModel, Field

import weave
from weave.scorers.llm_scorer import InstructorLLMScorer
from weave.scorers.llm_utils import OPENAI_DEFAULT_MODEL, create
from weave.scorers.utils import stringify

try:
    import torch
    from transformers import pipeline
except ImportError:
    import_failed = True
    print(
        "The `transformers` package is required to use the CoherenceScorer, please run `pip install transformers`"
    )

DEFAULT_HALLUCINATION_SYSTEM_PROMPT = """
Given some <input_data> from a user and an <output> generated by an AI system, \
determine if the <output> contains any hallucinations.

A "hallucination" is defined as information in the <output> that is not supported by \
the <input_data> or is not factually or logically consistent with the <input_data>.

# Steps
1. Carefully read and understand the input data.
2. Examine the model output.
3. Compare the output to the input data, identifying any inconsistencies or additions.
4. Evaluate the logical connection between input and output.
5. Determine if any information in the output is not supported by or conflicts with the input.

# Guidelines
- Focus on factual accuracy and logical consistency
- Consider both explicit and implicit information in the input data
- Be aware of potential misinterpretations or over-generalizations in the output
- Identify any information in the output that goes beyond the scope of the input

# Examples
## Data to analyze

<input_data_example>
The cat is black and white.
</input_data_example>

<output_example>
The cat has orange stripes.
</output_example>

## Analysis:
{
  "think_step_by_step": "The cat is black and white. The cat has orange stripes. \
The output contradicts the input data because the input specifies black and white, \
while the output mentions orange. The output also introduces a pattern not present in \
the input.",
  "reasoning": [
    {
      "hallucination_type": "Color comparison",
      "observation": "Input specifies black and white, output mentions orange"
    },
    {
      "hallucination_type": "Pattern analysis",
      "observation": "Input doesn't mention any pattern, output introduces stripes"
    }
  ],
  "conclusion": "The output contains two hallucinations: it contradicts the color information \
and introduces a pattern not present in the input."
  "is_hallucination": true,
}

# Notes
- Ensure each step in the reasoning process is clearly articulated
- Be objective and avoid assumptions not supported by the input data
- If the output contains factual information not present in the input, it may be a \
hallucination even if it doesn't directly contradict the input
"""

DEFAULT_HALLUCINATION_USER_PROMPT = """
Analyze the following <input_data> and <output> and determine if the <output> contains any hallucinations.
# Data to analyze

<input_data>
{input_data}
</input_data>

<output>
{output}
</output>
"""


def get_chat_template_messages(query:str, output:str, context:str=None):
    system_prompt = """The task is to evaluate whether the <output> contains \
information not supported by the <query> or <context>, or \
whether the <output> contradicts the information provided in the <query> or <context>.

<definitions>
The task is to evaluate if the <output>:
- Contains information that has no basis in the provided <query> or <context> (Faithfulness) OR
- Makes claims or statements that cannot be verified using the given <query> or <context> (Unsubstantiated Claim) OR
- Contradicts the information provided in the <query> or <context> (Contradiction)

<example_unsubstantiated_claim>
<query>What is the capital of France?</query>
<context>Paris is the capital of France.</context>
<output>Paris is the capital of France and has a population of 2.2 million people.</output>
</example_unsubstantiated_claim>

Explanation: The population information is baseless information as its not provided in the <context>.

<contradiction>
A contradiction occurs when the <output> directly conflicts with information provided in the <query> or <context>.

<example_contradiction>
<query>What is the speed limit on this highway?</query>
<context>The speed limit on Highway 101 is 65 mph.</context>
<output>The speed limit on Highway 101 is 55 mph.</output>
</example_contradiction>

Explanation: The output contradicts the speed limit stated in the context.
</contradiction>
</definitions>

<evaluation_criteria>
For each output, evaluate:
1. Faithfulness: Does the <output> strictly adhere to information present in the <query> or <context>? If not, return True.
2. Baseless Information: Does the <output> include details not supported by the <query> or <context>? If yes, return True.
3. Contradiction: Does the <output> contradict the information provided in the <query> or <context>? If yes, return True.
</evaluation_criteria>
"""

    prompt_template = """Does the following <output> meet the criteria for lack of Faithfulness, \
Unsubstantiated Claim or Contradiction?

<query>
{query}
</query>

<context>
{context}
</context>

<output>
{output}
</output>

Answer only with True or False.""".format(query=query, context=context, output=output)

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt_template}
    ]
   
    return messages



class HallucinationReasoning(BaseModel):
    hallucination_type: str = Field(
        description="A short name for the type of hallucination."
    )
    observation: str = Field(
        description="An observation from the <input_data> and <output> that supports the hallucination."
    )


class HallucinationResponse(BaseModel):
    chain_of_thought: str = Field(
        description="Think step by step about whether the <output> contains hallucinations \
based on the <input_data>."
    )
    reasonings: list[HallucinationReasoning] = Field(
        description="A list of reasoning steps that lead to the conclusion about whether or not\
the <output> contains hallucinations."
    )
    conclusion: str = Field(description="The conclusion of the analysis.")
    has_hallucination: bool = Field(
        description="Whether the <output> is free of hallucinations based on the <input_data>. True means it is NOT a hallucination."
    )


class HallucinationFreeScorer(InstructorLLMScorer):
    """
    A Scorer that uses an LLM to determine if the model output contains any hallucinations
    based on the input data.

    Note:
        - The meaning of "hallucination" can vary from person to person, you will likely want to
        customize the `system_prompt` and `user_prompt` to fit your specific needs.
        - This Scorer uses the `InstructorLLMScorer` class to generate structured outputs from the LLM
        provider's response; you will have to install the `instructor` python package to use it.
        - The `score` method expects the input column from the dataset to be named "context". It will use
        this data as the ground-truth to check hallucinations against. If your dataset column has a
        different name, you can specify a different mapping using the `column_map` argument in the init
        of HallucinationFreeScorer by passing `column_map={"context": "context"}`.

    Attributes:
        system_prompt (str): The prompt describing the task, defines what a "hallucination" is.
        user_prompt (str): The string template to pass the input and output data. The template must
        contain placeholders for both `{input_data}` and `{output}`.
        model_id (str): The LLM model name, depends on the LLM's providers to be used `client` being used.
        temperature (float): LLM temperature setting.
        max_tokens (int): Maximum number of tokens in the LLM's response.

    Methods:
        score(output: str, context: str) -> HallucinationResponse:
            Analyzes the output to detect hallucinations based on the given context.
    """

    system_prompt: str = DEFAULT_HALLUCINATION_SYSTEM_PROMPT
    user_prompt: str = DEFAULT_HALLUCINATION_USER_PROMPT
    model_id: str = OPENAI_DEFAULT_MODEL
    temperature: float = 0.7
    max_tokens: int = 4096

    @weave.op
    def score(self, output: str, context: str) -> HallucinationResponse:
        output = stringify(output)
        response = create(
            self.client,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {
                    "role": "user",
                    "content": self.user_prompt.format(
                        input_data=context, output=output
                    ),
                },
            ],
            model=self.model_id,
            response_model=HallucinationResponse,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
        )
        return response.model_dump()  # Morgan wants this to be a dict
    

from transformers import AutoModelForCausalLM, AutoTokenizer


class HallucinationScorer(weave.Scorer):
    """
    Nice docstring
    """
    device: str = "cpu"
    model_name_or_path: str = "wandb/coherence_scorer"
    base_url: Optional[str] = None
    llm_model: Any = None
    tokenizer: Any = None
    max_new_tokens: int = 2
    model_max_length:int = 8192
    device: str = "cuda"
    do_sample: bool = False
    temperature: float = 0.0
    num_beams: int = 1
    top_k: int = 20
    top_p: int = 0.7
    use_torch_compile: bool = False
    debug: bool = False
    
    def model_post_init(self, __context) -> None:
        if self.base_url:
            print(f"Using external API at {self.base_url} for scoring.")
            return  # Skip local model loading if base_url is provided
        
        if not torch.cuda.is_available() and "cuda" in self.device:
            raise ValueError("CUDA is not available")
        
        if self.llm_model is None:
            self.llm_model = AutoModelForCausalLM.from_pretrained(
                self.model_name_or_path, 
                torch_dtype="bfloat16"
            ).to(self.device)

            if self.use_torch_compile:
                self.llm_model.generation_config.cache_implementation = "static"
                self.llm_model = torch.compile(self.llm_model, backend="inductor", fullgraph=True)
        
        if self.tokenizer is None:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, 
                                                           model_max_length=self.model_max_length)
        if not self.do_sample:
            self.top_k = None
            self.top_p = None
        
    @weave.op
    def predict(self, query:str, context:str, output:str):
        messages = get_chat_template_messages(
            query=query,
            context=context,
            output=output,
            do_training=False
        )

        inp_template = self.tokenizer.apply_chat_template(
            messages,
            return_tensors="pt",
            tokenize=False,
            add_generation_prompt=True
        )
        inp_tokenized = self.tokenizer(inp_template, return_tensors="pt").to(self.device)
        
        pad_token_id = self.tokenizer.eos_token_id

        with torch.no_grad():            
            self.llm_model.eval()
            
            output = self.llm_model.generate(
                inp_tokenized["input_ids"],
                max_new_tokens=self.max_new_tokens,
                attention_mask=inp_tokenized["attention_mask"],
                pad_token_id=pad_token_id,
                temperature=self.temperature,
                do_sample=self.do_sample,
                num_beams=self.num_beams,
                top_k=self.top_k,
                top_p=self.top_p
            )

        true_token = 2787
        false_token = 4245

        input_length = inp_tokenized["input_ids"].shape[1]
        completion_tokens = output[0][input_length:].tolist()

        is_hallucination = (true_token in completion_tokens)
        result = {"is_hallucination": is_hallucination}
        
        if self.debug:
            scorer_worked = (completion_tokens.count(true_token) + completion_tokens.count(false_token)) == 1
            print(f"COMPLETION TOKENS: {completion_tokens}\n----------------------\n")
            completion = self.tokenizer.decode(completion_tokens)
            print(f"COMPLETION:\n{completion}\n----------------------\n")

            result.update({
                "completion": completion,
                "completion_tokens": completion_tokens,
                "total_tokens": len(output[0]),
                "total_completion_tokens": len(completion_tokens),
                "scorer_worked": scorer_worked
            })
        
        return result









class CoherenceScorer(Scorer):
    """
    Use wandb/coherence_scorer to check if the model output is coherent.

    Args:
        model_name: The name of the coherence scorer model to use. Defaults to `wandb/coherence_scorer`.
        device: The device to use for inference. Defaults to `cpu`.
    """

    device: str = "cpu"
    model_name_or_path: str = "wandb/coherence_scorer"
    base_url: Optional[str] = None
    _classifier: Any = PrivateAttr()
    _label2id: dict[str, int] = PrivateAttr()

    def model_post_init(self, __context: Any) -> None:
        if self.base_url:
            print(f"Using external API at {self.base_url} for scoring.")
            return  # Skip local model loading if base_url is provided
        if not torch.cuda.is_available() and "cuda" in self.device:
            raise ValueError("CUDA is not available")
        self._classifier = pipeline(
            task="sentiment-analysis", model=self.model_name_or_path, device=self.device
        )
        self._label2id = {
            "Completely Incoherent": 0,
            "Mostly Incoherent": 1,
            "A Little Incoherent": 2,
            "Mostly Coherent": 3,
            "Perfectly Coherent": 4,
        }

    @weave.op
    def score_messages(self, prompt: str, output: str) -> dict[str, Any]:
        """Score a prompt response pair."""
        coherence_output = self._classifier(
            inputs={"text": prompt, "text_pair": output}
        )
        coherent = True
        if "incoherent" in coherence_output["label"].lower():
            coherent = False

        return {
            "is_coherent": coherent,
            "coherence": coherence_output["label"],
            "coherence_score": self._label2id[coherence_output["label"]],
            "confidence": coherence_output["score"],
        }

    def _format_chat_history(self, chat_history: list[dict[str, str]]) -> str:
        """Format the chat history for the prompt."""
        formatted_chat_history = ""
        for turn in chat_history:
            if turn["role"] == "user":
                formatted_chat_history += f"{turn['text']}\n<extra_id_1>Assistant\n"
            else:
                formatted_chat_history += f"{turn['text']}\n<extra_id_1>User\n"
        return formatted_chat_history
    
    def _score_via_api(
            self, 
            input: str, 
            output: str, 
            chat_history: Optional[list[dict[str, str]]] = None, 
            context: Optional[str] = None
    ) -> dict[str, Any]:
        import requests
        response = requests.post(
            self.base_url,
            json={"input": input, "output": output, "chat_history": chat_history, "context": context}
        )
        response.raise_for_status()
        return response.json()
    
    @weave.op
    def score(
        self,
        input: str,
        output: str,
        chat_history: Optional[list[dict[str, str]]] = None,
        context: Optional[str] = None,
    ) -> dict[str, Any]:
        if self.base_url:
            return self._score_via_api(input, output, chat_history, context)
        prompt = input
        if chat_history is not None:
            history = self._format_chat_history(chat_history)
            prompt = f"{history}{input}"
        if context is not None:
            prompt = f"{input}\n\n{context}"
        return self.score_messages(prompt, output)
