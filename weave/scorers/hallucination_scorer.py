import os
from typing import Any, Optional

from pydantic import BaseModel, Field

import weave
from weave.scorers.base_scorer import Scorer
from weave.scorers.llm_scorer import InstructorLLMScorer
from weave.scorers.llm_utils import (
    OPENAI_DEFAULT_MODEL,
    create,
    download_model,
    scorer_model_paths,
)
from weave.scorers.utils import stringify

try:
    import torch
    from transformers import (
        AutoModelForCausalLM,
        AutoModelForSequenceClassification,
        AutoTokenizer,
    )
except ImportError:
    import_failed = True
    print(
        "The `transformers` package is required to use the HallucinationScorer, please run `pip install transformers`"
    )

DEFAULT_HALLUCINATION_SYSTEM_PROMPT = """
Given some <input_data> from a user and an <output> generated by an AI system, \
determine if the <output> contains any hallucinations.

A "hallucination" is defined as information in the <output> that is not supported by \
the <input_data> or is not factually or logically consistent with the <input_data>.

# Steps
1. Carefully read and understand the input data.
2. Examine the model output.
3. Compare the output to the input data, identifying any inconsistencies or additions.
4. Evaluate the logical connection between input and output.
5. Determine if any information in the output is not supported by or conflicts with the input.

# Guidelines
- Focus on factual accuracy and logical consistency
- Consider both explicit and implicit information in the input data
- Be aware of potential misinterpretations or over-generalizations in the output
- Identify any information in the output that goes beyond the scope of the input

# Examples
## Data to analyze

<input_data_example>
The cat is black and white.
</input_data_example>

<output_example>
The cat has orange stripes.
</output_example>

## Analysis:
{
  "think_step_by_step": "The cat is black and white. The cat has orange stripes. \
The output contradicts the input data because the input specifies black and white, \
while the output mentions orange. The output also introduces a pattern not present in \
the input.",
  "reasoning": [
    {
      "hallucination_type": "Color comparison",
      "observation": "Input specifies black and white, output mentions orange"
    },
    {
      "hallucination_type": "Pattern analysis",
      "observation": "Input doesn't mention any pattern, output introduces stripes"
    }
  ],
  "conclusion": "The output contains two hallucinations: it contradicts the color information \
and introduces a pattern not present in the input."
  "is_hallucination": true,
}

# Notes
- Ensure each step in the reasoning process is clearly articulated
- Be objective and avoid assumptions not supported by the input data
- If the output contains factual information not present in the input, it may be a \
hallucination even if it doesn't directly contradict the input
"""

DEFAULT_HALLUCINATION_USER_PROMPT = """
Analyze the following <input_data> and <output> and determine if the <output> contains any hallucinations.
# Data to analyze

<input_data>
{input_data}
</input_data>

<output>
{output}
</output>
"""


def get_chat_template_messages(query: str, output: str, context: str = None):
    system_prompt = """The task is to evaluate whether the <output> contains \
information not supported by the <query> or <context>, or \
whether the <output> contradicts the information provided in the <query> or <context>.

<definitions>
The task is to evaluate if the <output>:
- Contains information that has no basis in the provided <query> or <context> (Faithfulness) OR
- Makes claims or statements that cannot be verified using the given <query> or <context> (Unsubstantiated Claim) OR
- Contradicts the information provided in the <query> or <context> (Contradiction)

<example_unsubstantiated_claim>
<query>What is the capital of France?</query>
<context>Paris is the capital of France.</context>
<output>Paris is the capital of France and has a population of 2.2 million people.</output>
</example_unsubstantiated_claim>

Explanation: The population information is baseless information as its not provided in the <context>.

<contradiction>
A contradiction occurs when the <output> directly conflicts with information provided in the <query> or <context>.

<example_contradiction>
<query>What is the speed limit on this highway?</query>
<context>The speed limit on Highway 101 is 65 mph.</context>
<output>The speed limit on Highway 101 is 55 mph.</output>
</example_contradiction>

Explanation: The output contradicts the speed limit stated in the context.
</contradiction>
</definitions>

<evaluation_criteria>
For each output, evaluate:
1. Faithfulness: Does the <output> strictly adhere to information present in the <query> or <context>? If not, return True.
2. Baseless Information: Does the <output> include details not supported by the <query> or <context>? If yes, return True.
3. Contradiction: Does the <output> contradict the information provided in the <query> or <context>? If yes, return True.
</evaluation_criteria>
"""

    prompt_template = f"""Does the following <output> meet the criteria for lack of Faithfulness, \
Unsubstantiated Claim or Contradiction?

<query>
{query}
</query>

<context>
{context}
</context>

<output>
{output}
</output>

Answer only with True or False."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt_template},
    ]

    return messages


class HallucinationReasoning(BaseModel):
    hallucination_type: str = Field(
        description="A short name for the type of hallucination."
    )
    observation: str = Field(
        description="An observation from the <input_data> and <output> that supports the hallucination."
    )


class HallucinationResponse(BaseModel):
    chain_of_thought: str = Field(
        description="Think step by step about whether the <output> contains hallucinations \
based on the <input_data>."
    )
    reasonings: list[HallucinationReasoning] = Field(
        description="A list of reasoning steps that lead to the conclusion about whether or not\
the <output> contains hallucinations."
    )
    conclusion: str = Field(description="The conclusion of the analysis.")
    has_hallucination: bool = Field(
        description="Whether the <output> is free of hallucinations based on the <input_data>. True means it is NOT a hallucination."
    )


class HallucinationFreeScorer(InstructorLLMScorer):
    """
    A Scorer that uses an LLM to determine if the model output contains any hallucinations
    based on the input data.

    Note:
        - The meaning of "hallucination" can vary from person to person, you will likely want to
        customize the `system_prompt` and `user_prompt` to fit your specific needs.
        - This Scorer uses the `InstructorLLMScorer` class to generate structured outputs from the LLM
        provider's response; you will have to install the `instructor` python package to use it.
        - The `score` method expects the input column from the dataset to be named "context". It will use
        this data as the ground-truth to check hallucinations against. If your dataset column has a
        different name, you can specify a different mapping using the `column_map` argument in the init
        of HallucinationFreeScorer by passing `column_map={"context": "context"}`.

    Attributes:
        system_prompt (str): The prompt describing the task, defines what a "hallucination" is.
        user_prompt (str): The string template to pass the input and output data. The template must
        contain placeholders for both `{input_data}` and `{output}`.
        model_id (str): The LLM model name, depends on the LLM's providers to be used `client` being used.
        temperature (float): LLM temperature setting.
        max_tokens (int): Maximum number of tokens in the LLM's response.

    Methods:
        score(output: str, context: str) -> HallucinationResponse:
            Analyzes the output to detect hallucinations based on the given context.
    """

    system_prompt: str = DEFAULT_HALLUCINATION_SYSTEM_PROMPT
    user_prompt: str = DEFAULT_HALLUCINATION_USER_PROMPT
    model_id: str = OPENAI_DEFAULT_MODEL
    temperature: float = 0.7
    max_tokens: int = 4096

    @weave.op
    def score(self, output: str, context: str) -> HallucinationResponse:
        output = stringify(output)
        response = create(
            self.client,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {
                    "role": "user",
                    "content": self.user_prompt.format(
                        input_data=context, output=output
                    ),
                },
            ],
            model=self.model_id,
            response_model=HallucinationResponse,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
        )
        return response.model_dump()  # Morgan wants this to be a dict


class HallucinationScorer(Scorer):
    """
    A scorer that detects hallucinations in the output, given an query and context.

    This scorer uses a fine-tuned LLM to analyze whether model outputs contain information not supported
    by the given context.

    Args:
        device: Device to run model on, defaults to "cuda"
        model_name_or_path: Path or name of model weights to load
        base_url: Optional URL for external API scoring instead of local model
        debug: Enable debug logging, defaults to False
    """

    model_name_or_path: str = ""
    base_url: Optional[str] = None
    device: str = "cuda"
    debug: bool = False
    llm_model: Any = None
    tokenizer: Any = None
    max_new_tokens: int = 2
    model_max_length: int = 8192
    do_sample: bool = False
    temperature: float = 0.0
    num_beams: int = 1
    top_k: int = 20
    top_p: int = 0.7
    use_torch_compile: bool = False
    use_hhem: bool = True
    hhem_score_threshold: float = 0.5
    _local_model_path: str = None

    def model_post_init(self, __context) -> None:
        if self.base_url:
            print(f"Using external API at {self.base_url} for scoring.")
            return  # Skip local model loading if base_url is provided

        # torch.cuda.is_available()
        if not torch.cuda.is_available() and "cuda" in self.device:
            raise ValueError("CUDA is not available")

        if self.llm_model is None:
            # Check if the model is already downloaded
            if os.path.isdir(self.model_name_or_path):
                self._local_model_path = self.model_name_or_path
            # Else assume it's a wandb model name and download it
            else:
                if self.use_hhem:
                    self._local_model_path = download_model(
                        scorer_model_paths["hallucination_hhem_scorer"]
                    )
                else:
                    self._local_model_path = download_model(
                        scorer_model_paths["hallucination_scorer"]
                    )

            if self.use_hhem:
                self.llm_model = AutoModelForSequenceClassification.from_pretrained(
                    self._local_model_path,
                    torch_dtype="bfloat16",
                    trust_remote_code=True,
                ).to(self.device)
                self.tokenizer = self.llm_model.tokenzier
            else:
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    self._local_model_path, torch_dtype="bfloat16"
                ).to(self.device)

                if self.use_torch_compile:
                    self.llm_model.generation_config.cache_implementation = "static"
                    self.llm_model = torch.compile(
                        self.llm_model, backend="inductor", fullgraph=True
                    )

        if self.tokenizer is None:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self._local_model_path, model_max_length=self.model_max_length
            )
        if not self.do_sample:
            self.top_k = None
            self.top_p = None
            self.temperature = None

    def _score_via_api(self, messages: list) -> dict[str, Any]:
        import requests

        response = requests.post(self.base_url, json={"messages": messages})
        response.raise_for_status()
        return response.json()

    @weave.op
    def score(self, query: str, context: str, output: str) -> dict:
        messages = get_chat_template_messages(
            query=query,
            context=context,
            output=output,
        )
        if self.base_url:
            output = self._score_via_api(messages)
            output = output["data"]

        else:
            if self.use_hhem:
                pairs = [(query + "\n\n" + context, output)]
                pred = self.llm_model.predict(pairs)

                score = pred.item()
                return {
                    "flagged": score <= self.hhem_score_threshold,
                    "extras": {"score": score},
                }
            else:
                inp_template = self.tokenizer.apply_chat_template(
                    messages,
                    return_tensors="pt",
                    tokenize=False,
                    add_generation_prompt=True,
                )
                inp_tokenized = self.tokenizer(inp_template, return_tensors="pt").to(
                    self.device
                )

                pad_token_id = self.tokenizer.eos_token_id

                with torch.no_grad():
                    self.llm_model.eval()

                    output = self.llm_model.generate(
                        inp_tokenized["input_ids"],
                        max_new_tokens=self.max_new_tokens,
                        attention_mask=inp_tokenized["attention_mask"],
                        pad_token_id=pad_token_id,
                        temperature=self.temperature,
                        do_sample=self.do_sample,
                        num_beams=self.num_beams,
                        top_k=self.top_k,
                        top_p=self.top_p,
                    )

                true_token = 2787
                false_token = 4245

                input_length = inp_tokenized["input_ids"].shape[1]
                completion_tokens = output[0][input_length:].tolist()

                is_hallucination = true_token in completion_tokens
                result = {
                    "flagged": is_hallucination,
                    "extras": {"score": 1 if is_hallucination else 0},
                }

                if self.debug:
                    scorer_worked = (
                        completion_tokens.count(true_token)
                        + completion_tokens.count(false_token)
                    ) == 1
                    print(
                        f"COMPLETION TOKENS:\n{completion_tokens}\n----------------------\n"
                    )
                    completion = self.tokenizer.decode(completion_tokens)
                    print(f"COMPLETION:\n{completion}\n----------------------\n")

                    result["extras"].update(
                        {
                            "completion": completion,
                            "completion_tokens": completion_tokens,
                            "total_tokens": len(output[0]),
                            "total_completion_tokens": len(completion_tokens),
                            "scorer_worked": scorer_worked,
                        }
                    )

        return result
