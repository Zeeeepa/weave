# Predefined Scorers

This page provides an overview of Weave's predefined scorers, which support evaluation of AI models across various dimensions like hallucination detection, summarization quality, and content moderation. Predefined scorers currently support OpenAI, Anthropic, Google and MistralAI clients.

:::important
Predefined scorers are not available in Typescript. All usage instructions and code examples on this page are for Python. To create scorers in Typescript, see [function-based scorers](../evaluation/custom-scorers.md#function-based-scorers).
:::

To get started with predefined scorers, complete the following steps:

1. Learn how to [work with scorers](../../guides/evaluation/scorers.md#working-with-scorers)
2. Complete the [prerequisites](#prerequisites)
3. [Select the right predefined scorer](#select-a-predefined-scorer) for your use case.

## Prerequisites

Predefined scorers require additional dependencies:

```bash
pip install weave[scorers]
```

## Select a predefined scorer

When deciding which predefined scorer to use, consider the type of evaluation your AI system requires:

- [HallucinationFreeScorer](#hallucinationfreescorer):  
  Use if you need to identify whether your AI model generates hallucinations in its output. 
- [SummarizationScorer](#summarizationscorer):  
  Use if you need to evaluate the quality of summaries generated by your model. This scorer checks both the "information density" and the overall quality of the summary compared to the original text.
- [OpenAIModerationScorer](#openaimoderationscorer):  
  Use if you need to detect inappropriate content such as hate speech, violence, or explicit material in your model's output. This scorer uses OpenAI's Moderation API.
- [EmbeddingSimilarityScorer](#embeddingsimilarityscorer):  
  Use if you need to measure how similar your AI's output is to a reference text. This scorer calculates the cosine similarity between embeddings, making it useful for tasks requiring output fidelity.
- [ValidJSONScorer](#validjsonscorer):  
  Use if you need to verify that your AI model produces valid JSON output. Essential for ensuring structured data compliance.
- [ValidXMLScorer](#validxmlscorer):  
  Use if you need to check whether your AI system generates valid XML. This is particularly useful for scenarios involving XML-based data exchange or configuration.
- [PydanticScorer](#pydanticscorer):  
  Use if you need to validate the AI system's output against a predefined Pydantic schema to ensure adherence to a specific data structure or format.
- [ContextEntityRecallScorer](#contextentityrecallscorer):  
  Use if you need to assess whether your AI system accurately recalls key entities from the input context. Ideal for retrieval-augmented generation (RAG) systems.
- [ContextRelevancyScorer](#contextrelevancyscorer):  
  Use if you need to evaluate whether the provided context is relevant to the generated output. This scorer is especially valuable for ensuring that your model leverages relevant context effectively in RAG systems.
- [FaithfulnessScorer](#faithfulnessscorer):  
  Use if you need to verify that your AI model's output remains faithful to the provided input and context, ensuring factual consistency.
- [BiasScorer](#biasscorer):  
  Use if you need to detect biased or stereotypical content in your AI system's output. Ideal for reducing harmful biases in generated text.
- [ToxicityScorer](#toxicityscorer):  
  Use if you need to identify toxic or harmful content in your AI system's output, including hate speech or threats.
- [RelevanceScorer](#relevancescorer):  
  Use if you need to measure whether the AI system's output is relevant to the input and context provided.
- [CoherenceScorer](#coherencescorer):  
  Use if you need to evaluate the coherence and logical structure of the AI system's output.
- [RobustnessScorer](#robustnessscorer):  
  Use if you need to assess the robustness of your AI system by evaluating the consistency of its output across input variations.
- [PerplexityScorer](#perplexityscorer):  
  Use if you need to evaluate the perplexity of your AI system's output to measure language fluency and predictability.
- [AccuracyScorer](#accuracyscorer):  
  Use if you need to measure the accuracy of your AI system's predictions against ground truth labels for classification tasks.
- [RougeScorer](#rougescorer):  
  Use if you need to evaluate the quality of summaries generated by your model by comparing them to reference texts using ROUGE metrics.
- [BLEUScorer](#bleuscorer):  
  Use if you need to evaluate the quality of translations or paraphrased outputs by comparing them to reference texts using BLEU metrics.

Before you use one of the predefined scorers, ensure you meet the [prerequisites](#prerequisites).

### `HallucinationFreeScorer`

The `HallucinationFreeScorer` checks if your AI system's output includes any hallucinations based on the input data. For further usage information, see [Customization](#customization), [Usage notes](#usage-notes) and the [example](#example).

```python
from weave.scorers import HallucinationFreeScorer

llm_client = ... # initialize your LLM client here

scorer = HallucinationFreeScorer(
client=llm_client,
model_id="gpt-4o"
)
```

#### Customization

You can customize the `system_prompt` and `user_prompt` attributes of the scorer to define what qualifies as a hallucination.

#### Usage notes

The `score` method expects an input column named `context`. If your dataset uses a different name, [use the `column_map` attribute](../evaluation/scorers.md#mapping-column-names-with-column_map) to map `context` to the dataset column.

#### Example  

The following example shows the usage of `HallucinationFreeScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import HallucinationFreeScorer

# Initialize clients and scorers
llm_client = OpenAI()
hallucination_scorer = HallucinationFreeScorer(
client=llm_client,
model_id="gpt-4o",
column_map={"context": "input", "output": "other_col"}
)

# Create dataset
dataset = [
{"input": "John likes various types of cheese."},
{"input": "Pepe likes various types of cheese."},
]

@weave.op
def model(input: str) -> str:
return "The person's favorite cheese is cheddar."

# Run evaluation
evaluation = weave.Evaluation(
dataset=dataset,
scorers=[hallucination_scorer],
)
result = asyncio.run(evaluation.evaluate(model))
print(result)
# {'HallucinationFreeScorer': {'has_hallucination': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': 1.4395725727081299}}
```

---

### `SummarizationScorer`

Use an LLM to compare a summary to the original text and evaluate the quality of the summary. The `SummarizationScorer` evaluates summaries in two ways:

1. _Entity Density_: Checks the ratio of unique entities (e.g. names, places,things) mentioned in the summary to the total word count in the summary in order to estimate the "information density" of the summary. Uses an LLM to extract the entities. Similar to how entity density is used in the [Chain of Density paper available on Arxiv](https://arxiv.org/abs/2309.04269).

2. _Quality Grading_: Uses an LLM-evaluator to categorize the summary as one of `poor`, `ok`, or `excellent`. These grades are converted to scores so that the average score can be calculated, where `0.0` is `poor`, `0.5` is `ok`, and `1.0` is `excellent`.

For further usage information, see [Customization](#customization-1), [Usage notes](#usage-notes-1) and the [example](#example-1).

```python
from weave.scorers import SummarizationScorer

llm_client = ... # initialize your LLM client here

scorer = SummarizationScorer(
client=llm_client,
model_id="gpt-4o"
)
```

#### Customization

Adjust `summarization_evaluation_system_prompt` and `summarization_evaluation_prompt` to define what makes a good summary.

#### Usage notes

- This scorer uses the `InstructorLLMScorer` class.
- The `score` method expects the original text that was summarized to be present in the `input` column of the dataset. [Use the `column_map` class](../evaluation/scorers.md#mapping-column-names-with-column_map) attribute to map `input` to the correct dataset column if needed.

#### Example  

The following example shows the `SummarizationScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import SummarizationScorer

class SummarizationModel(weave.Model):
@weave.op()
async def predict(self, input: str) -> str:
return "This is a summary of the input text."

# Initialize clients and scorers
llm_client = OpenAI()
model = SummarizationModel()
summarization_scorer = SummarizationScorer(
client=llm_client,
model_id="gpt-4o",
)
# Create dataset
dataset = [
{"input": "The quick brown fox jumps over the lazy dog."},
{"input": "Artificial Intelligence is revolutionizing various industries."}
]

# Run evaluation
evaluation = weave.Evaluation(dataset=dataset, scorers=[summarization_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'SummarizationScorer': {'is_entity_dense': {'true_count': 0, 'true_fraction': 0.0}, 'summarization_eval_score': {'mean': 0.0}, 'entity_density': {'mean': 0.0}}, 'model_latency': {'mean': 6.210803985595703e-05}}
```

---

### `OpenAIModerationScorer`

The `OpenAIModerationScorer` uses OpenAI's Moderation API to check if the AI system's output contains disallowed content, such as hate speech or explicit material. The `OpenAIModerationScorer` works by sending the AI's output to the OpenAI Moderation endpoint. This endpoint returns a dictionary indicating whether the content is flagged or not, along with information about the categories involved. You must install the `openai` Python package to use `OpenAIModerationScorer`. For further usage information, see [Usage notes](#usage-notes-2) and the [example](#example-2).

```python
from weave.scorers import OpenAIModerationScorer
from openai import OpenAI

oai_client = OpenAI(api_key=...) # initialize your LLM client here

scorer = OpenAIModerationScorer(
client=oai_client,
model_id="text-embedding-3-small"
)
```

#### Usage notes

- You must install the `openai` Python package to use `OpenAIModerationScorer`.
- The client must be an instance of OpenAI's `OpenAI` or `AsyncOpenAI` client.

#### Example

The following example shows `OpenAIModerationScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import OpenAIModerationScorer

class MyModel(weave.Model):
@weave.op
async def predict(self, input: str) -> str:
return input

# Initialize clients and scorers
client = OpenAI()
model = MyModel()
moderation_scorer = OpenAIModerationScorer(client=client)

# Create dataset
dataset = [
{"input": "I love puppies and kittens!"},
{"input": "I hate everyone and want to hurt them."}
]

# Run evaluation
evaluation = weave.Evaluation(dataset=dataset, scorers=[moderation_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'OpenAIModerationScorer': {'flagged': {'true_count': 1, 'true_fraction': 0.5}, 'categories': {'violence': {'true_count': 1, 'true_fraction': 1.0}}}, 'model_latency': {'mean': 9.500980377197266e-05}}
```

---

### `EmbeddingSimilarityScorer`

The `EmbeddingSimilarityScorer` computes the cosine similarity between the embeddings of the AI system's output and a target text from your dataset. It's useful for measuring how similar the AI's output is to a reference text. The `EmbeddingSimilarityScorer` takes the following parameters:

- `target`: `EmbeddingSimilarityScorer`  expects a `target` column in your dataset. This column is used calculate the cosine similarity of the embeddings of the `target` column to the AI system output. If your dataset doesn't contain a column called `target`, you can [use the scorers `column_map` attribute](../evaluation/scorers.md#mapping-column-names-with-column_map) to map `target` to the appropriate column name in your dataset. For more information, see [Column Mapping](../evaluation/scorers.md).
- `threshold` (float): The minimum cosine similarity score between the embedding of the AI system output and the embdedding of the `target`, above which the 2 samples are considered "similar", (defaults to `0.5`). `threshold` can be in a range from -1 to 1:
- `1` indicates identical direction.
- `0` indicates orthogonal vectors.
- `-1` indicates opposite direction.

For further usage information, see [Usage notes](#usage-notes-3) and the [example](#example-3).

```python
from weave.scorers import EmbeddingSimilarityScorer

llm_client = ...  # initialise your LlM client

similarity_scorer = EmbeddingSimilarityScorer(
client=llm_client
target_column="reference_text",  # the dataset column to compare the output against
threshold=0.4  # the cosine similarity threshold to use
)
```

#### Usage notes

The correct cosine similarity threshold can fluctuate quite a lot depending on your use case. As such, it is advisable to experiment with different thresholds.

#### Example

The following example shows `EmbeddingSimilarityScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import EmbeddingSimilarityScorer

# Initialize clients and scorers
client = OpenAI()
similarity_scorer = EmbeddingSimilarityScorer(
client=client,
threshold=0.7,
column_map={"target": "reference"}
)

# Create dataset
dataset = [
{
"input": "He's name is John",
"reference": "John likes various types of cheese.",
},
{
"input": "He's name is Pepe.",
"reference": "Pepe likes various types of cheese.",
},
]

# Define model
@weave.op
def model(input: str) -> str:
return "John likes various types of cheese."

# Run evaluation
evaluation = weave.Evaluation(
dataset=dataset,
scorers=[similarity_scorer],
)
result = asyncio.run(evaluation.evaluate(model))
print(result)
# {'EmbeddingSimilarityScorer': {'is_similar': {'true_count': 1, 'true_fraction': 0.5}, 'similarity_score': {'mean': 0.8448514031462045}}, 'model_latency': {'mean': 0.45862746238708496}}
```

---

### `ValidJSONScorer`

The `ValidJSONScorer` checks whether the AI system's output is valid JSON or not. This is useful if you expect the output to be in JSON format. 

```python
import asyncio
import weave
from weave.scorers import ValidJSONScorer

class JSONModel(weave.Model):
@weave.op()
async def predict(self, input: str) -> str:
# This is a placeholder.
# In a real scenario, this would generate JSON.
return '{"key": "value"}'

model = JSONModel()
json_scorer = ValidJSONScorer()

dataset = [
{"input": "Generate a JSON object with a key and value"},
{"input": "Create an invalid JSON"}
]

evaluation = weave.Evaluation(dataset=dataset, scorers=[json_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'ValidJSONScorer': {'json_valid': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': 8.58306884765625e-05}}
```

---

### `ValidXMLScorer`

The `ValidXMLScorer` checks whether the AI system's output is valid XML. This is useful if you are expecting XML-formatted outputs.

```python
import asyncio
import weave
from weave.scorers import ValidXMLScorer

class XMLModel(weave.Model):
@weave.op()
async def predict(self, input: str) -> str:
# This is a placeholder. In a real scenario, this would generate XML.
return '<root><element>value</element></root>'

model = XMLModel()
xml_scorer = ValidXMLScorer()

dataset = [
{"input": "Generate a valid XML with a root element"},
{"input": "Create an invalid XML"}
]

evaluation = weave.Evaluation(dataset=dataset, scorers=[xml_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'ValidXMLScorer': {'xml_valid': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': 8.20159912109375e-05}}
```

---

### `PydanticScorer`

The `PydanticScorer` validates the AI system's output against a Pydantic model to ensure it adheres to a specified schema or data structure.

```python
from weave.scorers import PydanticScorer
from pydantic import BaseModel

class FinancialReport(BaseModel):
revenue: int
year: str

pydantic_scorer = PydanticScorer(model=FinancialReport)
```

---

### `ContextEntityRecallScorer`

:::note
The `ContextEntityRecallScorer` is based on the [RAGAS](https://github.com/explodinggradients/ragas) evaluation library
:::

The `ContextEntityRecallScorer` uses an LLM to extract unique entities from the AI system's output and the provided context. It then calculates _recall_, which measures the proportion of contextually important entities that are captured in the output. This measurement helps to assess the model's effectiveness in retrieving relevant information. The `ContextEntityRecallScorer` returns a dictionary containing the recall score. For further usage information, see [Usage notes](#usage-notes-4) and the [example](#example-4).

```python
from weave.scorers import ContextEntityRecallScorer

llm_client = ...  # initialise your LlM client

entity_recall_scorer = ContextEntityRecallScorer(
client=llm_client
model_id="your-model-id"
)
```

#### Usage notes

`ContextEntityRecallScorer` expects a `context` column in your dataset. You can [use `column_map`](../evaluation/scorers.md#mapping-column-names-with-column_map) to map `context` to another dataset column if needed.

#### Example

See the [`ContextRelevancyScorer` example](#example-5).

---

### `ContextRelevancyScorer`

:::note
The `ContextRelevancyScorer` is based on the [RAGAS](https://github.com/explodinggradients/ragas) evaluation library
:::

The `ContextRelevancyScorer` evaluates the relevancy of the provided context to the AI system's output. It helps to determine if the context used is appropriate for generating the output. The works by using an LLM to rate the relevancy of the context to the output. The rating scale is from `0` to `1`, with `0` being least relevant and `1` being most relevant. `ContextRelevancyScorer` then returns a dictionary with the `relevancy_score`. For further usage information, see [Usage notes](#usage-notes-5) and the [example](#example-4).


```python
from weave.scorers import ContextRelevancyScorer

llm_client = ...  # initialise your LlM client

relevancy_scorer = ContextRelevancyScorer(
llm_client = ...  # initialise your LlM client
model_id="your-model-id"
)
```

#### Usage notes

- `ContextRelevancyScorer` expects a `context` column in your dataset. You can [use `column_map`](../evaluation/scorers.md#mapping-column-names-with-column_map) to map `context` to another dataset column if needed.
- Customize the `relevancy_prompt` to define how relevancy is assessed.

#### Example

The following example shows `ContextEntityRecallScorer` and `ContextRelevancyScorer` in the context of an evaluation:

```python
import asyncio
from textwrap import dedent
from openai import OpenAI
import weave
from weave.scorers import ContextEntityRecallScorer, ContextRelevancyScorer

class RAGModel(weave.Model):
@weave.op()
async def predict(self, question: str) -> str:
"Retrieve relevant context"
return "Paris is the capital of France."


model = RAGModel()

# Define prompts
relevancy_prompt: str = dedent("""
Given the following question and context, rate the relevancy of the context to the question on a scale from 0 to 1.

Question: {question}
Context: {context}
Relevancy Score (0-1):
""")

# Initialize clients and scorers
llm_client = OpenAI()
entity_recall_scorer = ContextEntityRecallScorer(
client=client,
model_id="gpt-4o",
)

relevancy_scorer = ContextRelevancyScorer(
client=llm_client,
model_id="gpt-4o",
relevancy_prompt=relevancy_prompt
)

# Create dataset
dataset = [
{
"question": "What is the capital of France?",
"context": "Paris is the capital city of France."
},
{
"question": "Who wrote Romeo and Juliet?",
"context": "William Shakespeare wrote many famous plays."
}
]

# Run evaluation
evaluation = weave.Evaluation(
dataset=dataset,
scorers=[entity_recall_scorer, relevancy_scorer]
)
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'ContextEntityRecallScorer': {'recall': {'mean': 0.3333333333333333}}, 'ContextRelevancyScorer': {'relevancy_score': {'mean': 0.5}}, 'model_latency': {'mean': 9.393692016601562e-05}}
```

### `FaithfulnessScorer`

The `FaithfulnessScorer` evaluates whether the AI system's output is consistent with the input query and context. 

#### Example

The following example shows how to use `FaithfulnessScorer` to evaluate the faithfulness of an AI system's output:

```python
from weave.scorers import FaithfulnessScorer

faithfulness_scorer = FaithfulnessScorer(model_name_or_path="models/faithfulness_scorer")

result = faithfulness_scorer.score(
  query="What is the capital of Antarctica?",
  context="People in Antarctica love the penguins.",
  output="The capital of Antarctica is Penguin City."
)
print(f"Output is not faithful: {result['flagged']}")
```

#### Usage notes
- Requires both `query` and `context` for evaluation.
- The `model_name_or_path` parameter points to the pre-trained model weights.

### `BiasScorer`
The `BiasScorer` identifies biased or stereotypical content in the AI system's output.

#### Example
The following example demonstrates the use of `BiasScorer` to detect bias:

```python
from weave.scorers import BiasScorer

bias_scorer = BiasScorer(model_name_or_path="models/bias_scorer")

result = bias_scorer.score("Men are terrible at cleaning.")
print(f"The input is biased: {result['flagged']}")
```

#### Usage notes
- Adjust thresholds to control the sensitivity to bias detection.
- Requires a `model_name_or_path` to specify the pre-trained model.

### `ToxicityScorer`
The `ToxicityScorer` evaluates the AI system's output for toxic or harmful content.

#### Example
The following example shows how to use `ToxicityScorer` to flag toxic content:

```python
from weave.scorers import ToxicityScorer

toxicity_scorer = ToxicityScorer(model_name_or_path="models/toxicity_scorer")

result = toxicity_scorer.score("People from Ireland are the worst.")
print(f"Input is toxic: {result['flagged']}")
```

#### Usage notes
- Scores for individual toxicity categories and a cumulative total are provided.
- Customize thresholds for stricter or more lenient detection.

### `RelevanceScorer`
The `RelevanceScorer` determines whether the AI system's output is relevant to the input and optional context.

#### Example
The following example demonstrates the use of `RelevanceScorer` to evaluate relevance:

```python
from weave.scorers import RelevanceScorer

relevance_scorer = RelevanceScorer(model_name_or_path="models/relevance_scorer")

result = relevance_scorer.score(
  input="What is the capital of Antarctica?",
  context="Antarctica has the happiest penguins.",
  output="The savannah has the biggest lions."
)
print(f"Output is relevant: {result['is_relevant']}")
```

#### Usage notes
- The scorer provides a binary `is_relevant` result and a numerical relevance score.
- `context` is optional, but can improve accuracy.

### `CoherenceScorer`
The `CoherenceScorer` evaluates whether the AI system's output is coherent and logically structured.

#### Example
The following example demonstrates the use of CoherenceScorer to evaluate output coherence:

```python
from weave.scorers import CoherenceScorer

coherence_scorer = CoherenceScorer(model_name_or_path="models/coherence_scorer")

result = coherence_scorer.score(
  input="What is the capital of Antarctica?",
  output="but why not monkey up day"
)
print(f"Output is coherent: {result['is_coherent']}")
```

#### Usage notes
- Designed to identify nonsensical or irrelevant outputs.
Outputs that fail coherence checks are flagged as incoherent.
- Define specific coherence parameters to handle task-specific output structures.

### `RobustnessScorer`
The `RobustnessScorer` measures the consistency of the AI system's output across variations of the same input.

#### Example
The following example shows how to use `RobustnessScorer` to test output robustness:

```python
from weave.scorers import RobustnessScorer

robustness_scorer = RobustnessScorer(use_exact_match=False, return_interpretation=True)

outputs = [
  "James Watt improved the steam engine in 1769, making it efficient enough for industrial use.",
  "In 1769, James Watt modified the steam engine to achieve better industrial efficiency.",
]

result = robustness_scorer.score(output=outputs)
print(result)
```

#### Usage notes
- Supports both exact match and semantic evaluation modes.
Useful for testing model stability under input perturbations.

### `PerplexityScorer`
The `PerplexityScorer` evaluates the _perplexity_ of the AI system's output, which is a measure of how well the model predicts the sequence.

#### Example
The following example shows how to compute perplexity using `PerplexityScorer`:

```python
from weave.scorers import HuggingFacePerplexityScorer

perplexity_scorer = HuggingFacePerplexityScorer()

result = perplexity_scorer.score(output="This is a sample output text.")
print(f"Perplexity score: {result['perplexity']}")
```

#### Usage notes
- Lower perplexity scores indicate higher confidence in the output.
- Useful for assessing the fluency of generated text.

### `AccuracyScorer`
The `AccuracyScorer` calculates the accuracy of the AI system's predictions against ground truth labels.

#### Example
The following example demonstrates how to calculate accuracy using `AccuracyScorer`:

```python
from weave.scorers import AccuracyScorer

accuracy_scorer = AccuracyScorer(task="binary")

ground_truths = [0, 0, 1]
outputs = [1, 0, 1]

result = accuracy_scorer.score(ground_truth=ground_truths, output=outputs)
print(result)
```

#### Usage notes
- Supports binary and multiclass classification tasks.
- Requires task-specific configurations.

### `RougeScorer`
The `RougeScorer` evaluates the quality of the AI system's summaries by comparing them to reference texts using [ROUGE metrics](https://en.wikipedia.org/wiki/ROUGE_(metric)).

#### Example
The following example shows how to use `RougeScorer` for summary evaluation:

```python
from weave.scorers import RougeScorer

rouge_scorer = RougeScorer()

result = rouge_scorer.score(
  ground_truth="The cat sat on the mat.",
  output="The cat is sitting on the mat."
)
print(result)
```

### `BLEUScorer`
The `BLEUScorer` evaluates the quality of translations or generated text by comparing them to reference texts using [BLEU metrics](https://en.wikipedia.org/wiki/BLEU).

#### Example
The following example demonstrates how to use `BLEUScorer`:

```python
from weave.scorers import BLEUScorer

bleu_scorer = BLEUScorer()

result = bleu_scorer.score(
  ground_truths=["The watermelon seeds will be excreted."],
  output="The watermelon seeds pass through your digestive system."
)
print(result)
```
