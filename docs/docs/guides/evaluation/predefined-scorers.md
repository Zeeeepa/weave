# Predefined scorers

This page provides an overview of Weave's predefined scorers, which support evaluation of AI models across various dimensions like hallucination detection, summarization quality, and content moderation. Predefined scorers currently support OpenAI, Anthropic, Google and MistralAI clients.

:::note
This feature is not available in Typescript. All usage instructions and code examples on this page are for Python.
:::

## Select a predefined scorer

When deciding which predefined scorer to use, consider the type of evaluation your AI system requires:

- [HallucinationFreeScorer](#hallucinationfreescorer):
  Use if you need to identify whether your AI model generates hallucinations in its output. 

- [SummarizationScorer](#summarizationscorer): 
  Use if you need to evaluate the quality of summaries generated by your model. This scorer checks both the "information density" and the overall quality of the summary compared to the original text.

- [OpenAIModerationScorer](#openaimoderationscorer): 
  Use if you need to detect inappropriate content such as hate speech, violence, or explicit material in your model's output. This scorer OpenAI's Moderation API.

- [EmbeddingSimilarityScorer](#embeddingsimilarityscorer):  
  Use if you need to measure how similar your AI's output is to a reference text. This scorer calculates the cosine similarity between embeddings, making it useful for tasks requiring output fidelity.

- [ValidJSONScorer](#validjsonscorer):
  Use if you need to verify that your AI model produces valid JSON output. Essential for ensuring structured data compliance.

- [ValidXMLScorer](#validxmlscorer):
  Use if you need to check whether your AI system generates valid XML. This is particularly useful for scenarios involving XML-based data exchange or configuration.

- [PydanticScorer](#pydanticscorer):
  Use if you need to validate the AI system's output against a predefined Pydantic schema to ensure adherence to a specific data structure or format.

- [ContextEntityRecallScorer](#contextentityrecallscorer): 
  Use if you need to assess whether your AI system accurately recalls key entities from the input context. Ideal for retrieval-augmented generation (RAG) systems.

- [ContextRelevancyScorer](#contextrelevancyscorer):
  Use if you need to evaluate whether the provided context is relevant to the generated output. This scorer is especially valuable for ensuring that your model leverages relevant context effectively in RAG systems.

## Prerequisites

Weave's predefined scorers require additional dependencies:

```bash
pip install weave[scorers]
```

## Available predefinedscorers

### `HallucinationFreeScorer`

The `HallucinationFreeScorer` checks if your AI system's output includes any hallucinations based on the input data. For further usage information, see [Customization](#customization), [Usage notes](#usage-notes) and the [example](#example).

```python
from weave.scorers import HallucinationFreeScorer

llm_client = ... # initialize your LLM client here

scorer = HallucinationFreeScorer(
client=llm_client,
model_id="gpt-4o"
)
```

#### Customization

You can customize the `system_prompt` and `user_prompt` attributes of the scorer to define what qualifies as a hallucination.

#### Usage notes

The `score` method expects an input column named `context`. If your dataset uses a different name, [use the `column_map` attribute](../evaluation/scorers.md#mapping-column-names-with-column_map) to map `context` to the dataset column.

#### Example  

The following example shows the usage of `HallucinationFreeScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import HallucinationFreeScorer

# Initialize clients and scorers
llm_client = OpenAI()
hallucination_scorer = HallucinationFreeScorer(
client=llm_client,
model_id="gpt-4o",
column_map={"context": "input", "output": "other_col"}
)

# Create dataset
dataset = [
{"input": "John likes various types of cheese."},
{"input": "Pepe likes various types of cheese."},
]

@weave.op
def model(input: str) -> str:
return "The person's favorite cheese is cheddar."

# Run evaluation
evaluation = weave.Evaluation(
dataset=dataset,
scorers=[hallucination_scorer],
)
result = asyncio.run(evaluation.evaluate(model))
print(result)
# {'HallucinationFreeScorer': {'has_hallucination': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': 1.4395725727081299}}
```

---

### `SummarizationScorer`

Use an LLM to compare a summary to the original text and evaluate the quality of the summary. The `SummarizationScorer` evaluates summaries in two ways:

1. _Entity Density_: Checks the ratio of unique entities (e.g. names, places,things) mentioned in the summary to the total word count in the summary in order to estimate the "information density" of the summary. Uses an LLM to extract the entities. Similar to how entity density is used in the [Chain of Density paper available on Arxiv](https://arxiv.org/abs/2309.04269).

2. _Quality Grading_: Uses an LLM-evaluator to categorize the summary as one of `poor`, `ok`, or `excellent`. These grades are converted to scores so that the average score can be calculated, where `0.0` is `poor`, `0.5` is `ok`, and `1.0` is `excellent`.

For further usage information, see [Customization](#customization-1), [Usage notes](#usage-notes-1) and the [example](#example-1).

```python
from weave.scorers import SummarizationScorer

llm_client = ... # initialize your LLM client here

scorer = SummarizationScorer(
client=llm_client,
model_id="gpt-4o"
)
```

#### Customization

Adjust `summarization_evaluation_system_prompt` and `summarization_evaluation_prompt` to define what makes a good summary.

#### Usage notes

- This scorer uses the `InstructorLLMScorer` class.
- The `score` method expects the original text that was summarized to be present in the `input` column of the dataset. [Use the `column_map` class](../evaluation/scorers.md#mapping-column-names-with-column_map) attribute to map `input` to the correct dataset column if needed.

#### Example  

The following example shows the `SummarizationScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import SummarizationScorer

class SummarizationModel(weave.Model):
@weave.op()
async def predict(self, input: str) -> str:
return "This is a summary of the input text."

# Initialize clients and scorers
llm_client = OpenAI()
model = SummarizationModel()
summarization_scorer = SummarizationScorer(
client=llm_client,
model_id="gpt-4o",
)
# Create dataset
dataset = [
{"input": "The quick brown fox jumps over the lazy dog."},
{"input": "Artificial Intelligence is revolutionizing various industries."}
]

# Run evaluation
evaluation = weave.Evaluation(dataset=dataset, scorers=[summarization_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'SummarizationScorer': {'is_entity_dense': {'true_count': 0, 'true_fraction': 0.0}, 'summarization_eval_score': {'mean': 0.0}, 'entity_density': {'mean': 0.0}}, 'model_latency': {'mean': 6.210803985595703e-05}}
```

---

### `OpenAIModerationScorer`

The `OpenAIModerationScorer` uses OpenAI's Moderation API to check if the AI system's output contains disallowed content, such as hate speech or explicit material. The `OpenAIModerationScorer` works by sending the AI's output to the OpenAI Moderation endpoint. This endpoint returns a dictionary indicating whether the content is flagged or not, along with information about the categories involved. You must install the `openai` Python package to use `OpenAIModerationScorer`. For further usage information, see [Usage notes](#usage-notes-2) and the [example](#example-2).

```python
from weave.scorers import OpenAIModerationScorer
from openai import OpenAI

oai_client = OpenAI(api_key=...) # initialize your LLM client here

scorer = OpenAIModerationScorer(
client=oai_client,
model_id="text-embedding-3-small"
)
```

#### Usage notes

- You must install the `openai` Python package to use `OpenAIModerationScorer`.
- The client must be an instance of OpenAI's `OpenAI` or `AsyncOpenAI` client.

#### Example

The following example shows `OpenAIModerationScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import OpenAIModerationScorer

class MyModel(weave.Model):
@weave.op
async def predict(self, input: str) -> str:
return input

# Initialize clients and scorers
client = OpenAI()
model = MyModel()
moderation_scorer = OpenAIModerationScorer(client=client)

# Create dataset
dataset = [
{"input": "I love puppies and kittens!"},
{"input": "I hate everyone and want to hurt them."}
]

# Run evaluation
evaluation = weave.Evaluation(dataset=dataset, scorers=[moderation_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'OpenAIModerationScorer': {'flagged': {'true_count': 1, 'true_fraction': 0.5}, 'categories': {'violence': {'true_count': 1, 'true_fraction': 1.0}}}, 'model_latency': {'mean': 9.500980377197266e-05}}
```

---

### `EmbeddingSimilarityScorer`

The `EmbeddingSimilarityScorer` computes the cosine similarity between the embeddings of the AI system's output and a target text from your dataset. It's useful for measuring how similar the AI's output is to a reference text. The `EmbeddingSimilarityScorer` takes the following parameters:

- `target`: `EmbeddingSimilarityScorer`  expects a `target` column in your dataset. This column is used calculate the cosine similarity of the embeddings of the `target` column to the AI system output. If your dataset doesn't contain a column called `target`, you can [use the scorers `column_map` attribute](../evaluation/scorers.md#mapping-column-names-with-column_map) to map `target` to the appropriate column name in your dataset. For more information, see [Column Mapping](../evaluation/scorers.md).
- `threshold` (float): The minimum cosine similarity score between the embedding of the AI system output and the embdedding of the `target`, above which the 2 samples are considered "similar", (defaults to `0.5`). `threshold` can be in a range from -1 to 1:
- `1` indicates identical direction.
- `0` indicates orthogonal vectors.
- `-1` indicates opposite direction.

For further usage information, see [Usage notes](#usage-notes-3) and the [example](#example-3).

```python
from weave.scorers import EmbeddingSimilarityScorer

llm_client = ...  # initialise your LlM client

similarity_scorer = EmbeddingSimilarityScorer(
client=llm_client
target_column="reference_text",  # the dataset column to compare the output against
threshold=0.4  # the cosine similarity threshold to use
)
```

#### Usage notes

The correct cosine similarity threshold can fluctuate quite a lot depending on your use case. As such, it is advisable to experiment with different thresholds.

#### Example

The following example shows `EmbeddingSimilarityScorer` in the context of an evaluation:

```python
import asyncio
from openai import OpenAI
import weave
from weave.scorers import EmbeddingSimilarityScorer

# Initialize clients and scorers
client = OpenAI()
similarity_scorer = EmbeddingSimilarityScorer(
client=client,
threshold=0.7,
column_map={"target": "reference"}
)

# Create dataset
dataset = [
{
"input": "He's name is John",
"reference": "John likes various types of cheese.",
},
{
"input": "He's name is Pepe.",
"reference": "Pepe likes various types of cheese.",
},
]

# Define model
@weave.op
def model(input: str) -> str:
return "John likes various types of cheese."

# Run evaluation
evaluation = weave.Evaluation(
dataset=dataset,
scorers=[similarity_scorer],
)
result = asyncio.run(evaluation.evaluate(model))
print(result)
# {'EmbeddingSimilarityScorer': {'is_similar': {'true_count': 1, 'true_fraction': 0.5}, 'similarity_score': {'mean': 0.8448514031462045}}, 'model_latency': {'mean': 0.45862746238708496}}
```

---

### `ValidJSONScorer`

The `ValidJSONScorer` checks whether the AI system's output is valid JSON or not. This is useful if you expect the output to be in JSON format. 

```python
import asyncio
import weave
from weave.scorers import ValidJSONScorer

class JSONModel(weave.Model):
@weave.op()
async def predict(self, input: str) -> str:
# This is a placeholder.
# In a real scenario, this would generate JSON.
return '{"key": "value"}'

model = JSONModel()
json_scorer = ValidJSONScorer()

dataset = [
{"input": "Generate a JSON object with a key and value"},
{"input": "Create an invalid JSON"}
]

evaluation = weave.Evaluation(dataset=dataset, scorers=[json_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'ValidJSONScorer': {'json_valid': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': 8.58306884765625e-05}}
```

---

### `ValidXMLScorer`

The `ValidXMLScorer` checks whether the AI system's output is valid XML. This is useful if you are expecting XML-formatted outputs.

```python
import asyncio
import weave
from weave.scorers import ValidXMLScorer

class XMLModel(weave.Model):
@weave.op()
async def predict(self, input: str) -> str:
# This is a placeholder. In a real scenario, this would generate XML.
return '<root><element>value</element></root>'

model = XMLModel()
xml_scorer = ValidXMLScorer()

dataset = [
{"input": "Generate a valid XML with a root element"},
{"input": "Create an invalid XML"}
]

evaluation = weave.Evaluation(dataset=dataset, scorers=[xml_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'ValidXMLScorer': {'xml_valid': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': 8.20159912109375e-05}}
```

---

### `PydanticScorer`

The `PydanticScorer` validates the AI system's output against a Pydantic model to ensure it adheres to a specified schema or data structure.

```python
from weave.scorers import PydanticScorer
from pydantic import BaseModel

class FinancialReport(BaseModel):
revenue: int
year: str

pydantic_scorer = PydanticScorer(model=FinancialReport)
```

---

### `ContextEntityRecallScorer`

:::note
The `ContextEntityRecallScorer` is based on the [RAGAS](https://github.com/explodinggradients/ragas) evaluation library
:::

The `ContextEntityRecallScorer` uses an LLM to extract unique entities from the AI system's output and the provided context. It then calculates _recall_, which measures the proportion of contextually important entities that are captured in the output. This measurement helps to assess the model's effectiveness in retrieving relevant information. The `ContextEntityRecallScorer` returns a dictionary containing the recall score. For further usage information, see [Usage notes](#usage-notes-4) and the [example](#example-4).

```python
from weave.scorers import ContextEntityRecallScorer

llm_client = ...  # initialise your LlM client

entity_recall_scorer = ContextEntityRecallScorer(
client=llm_client
model_id="your-model-id"
)
```

#### Usage notes

`ContextEntityRecallScorer` expects a `context` column in your dataset. You can [use `column_map`](../evaluation/scorers.md#mapping-column-names-with-column_map) to map `context` to another dataset column if needed.

#### Example

See the [`ContextRelevancyScorer` example](#example-6).

---

### `ContextRelevancyScorer`

:::note
The `ContextRelevancyScorer` is based on the [RAGAS](https://github.com/explodinggradients/ragas) evaluation library
:::

The `ContextRelevancyScorer` evaluates the relevancy of the provided context to the AI system's output. It helps tp determine if the context used is appropriate for generating the output. The works by using an LLM to rate the relevancy of the context to the output. The rating scale is from `0` to `1`, with `0` being least relevant and `1` being most relevant. `ContextRelevancyScorer` then returns a dictionary with the `relevancy_score`. For further usage information, see [Usage notes](#usage-notes-5) and the [example](#example-5).


```python
from weave.scorers import ContextRelevancyScorer

llm_client = ...  # initialise your LlM client

relevancy_scorer = ContextRelevancyScorer(
llm_client = ...  # initialise your LlM client
model_id="your-model-id"
)
```

#### Usage notes

- `ContextRelevancyScorer` expects a `context` column in your dataset. You can [use `column_map`](../evaluation/scorers.md#mapping-column-names-with-column_map) to map `context` to another dataset column if needed.
- Customize the `relevancy_prompt` to define how relevancy is assessed.

#### Example

The following example shows `ContextEntityRecallScorer` and `ContextRelevancyScorer` in the context of an evaluation:

```python
import asyncio
from textwrap import dedent
from openai import OpenAI
import weave
from weave.scorers import ContextEntityRecallScorer, ContextRelevancyScorer

class RAGModel(weave.Model):
@weave.op()
async def predict(self, question: str) -> str:
"Retrieve relevant context"
return "Paris is the capital of France."


model = RAGModel()

# Define prompts
relevancy_prompt: str = dedent("""
Given the following question and context, rate the relevancy of the context to the question on a scale from 0 to 1.

Question: {question}
Context: {context}
Relevancy Score (0-1):
""")

# Initialize clients and scorers
llm_client = OpenAI()
entity_recall_scorer = ContextEntityRecallScorer(
client=client,
model_id="gpt-4o",
)

relevancy_scorer = ContextRelevancyScorer(
client=llm_client,
model_id="gpt-4o",
relevancy_prompt=relevancy_prompt
)

# Create dataset
dataset = [
{
"question": "What is the capital of France?",
"context": "Paris is the capital city of France."
},
{
"question": "Who wrote Romeo and Juliet?",
"context": "William Shakespeare wrote many famous plays."
}
]

# Run evaluation
evaluation = weave.Evaluation(
dataset=dataset,
scorers=[entity_recall_scorer, relevancy_scorer]
)
results = asyncio.run(evaluation.evaluate(model))
print(results)
# {'ContextEntityRecallScorer': {'recall': {'mean': 0.3333333333333333}}, 'ContextRelevancyScorer': {'relevancy_score': {'mean': 0.5}}, 'model_latency': {'mean': 9.393692016601562e-05}}
```
